# create or update hive-site.xml : Global Configuration File for Hive
/opt/hive/conf/hive-site.xml
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:postgresql://localhost:6432/metastore</value>
    <description>JDBC Driver Connection for PostgrSQL</description>
  </property>
 
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.postgresql.Driver</value>
    <description>PostgreSQL metastore driver class name</description>
  </property>
 
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>Database User Name</description>
  </property>
 
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>sibaram12</value>
    <description>Database User Password</description>
  </property>
</configuration>
 
# Remove the conflicting Guava Files if present.
rm /opt/hive/lib/guava-19.0.jar
cp /opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar /opt/hive/lib/

# Download a postgresql jar file and copy it to /opt/hive/lib/
wget https://jdbc.postgresql.org/download/postgresql-42.2.24.jar
sudo mv postgresql-42.2.24.jar /opt/hive/lib/postgresql-42.2.24.jar

# Initialize Hive Metastore
schematool -dbType postgres -initSchema

# Validate Metadata Tables
docker exec \
    -it postgress_container \
    psql -U postgres \
    -d metastore


# Update /opt/spark3/conf/spark-defaults.conf with below properties.
spark.driver.extraJavaOptions     -Dderby.system.home=/tmp/derby/
spark.sql.repl.eagerEval.enabled  true
spark.master                      yarn
spark.eventLog.enabled            true
spark.eventLog.dir                hdfs:///spark3-logs
spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory     hdfs:///spark3-logs
spark.history.fs.update.interval  10s
spark.history.ui.port             18080
spark.yarn.historyServer.address  localhost:18080
spark.yarn.jars                   hdfs:///spark3-jars/*.jar

# Create directories for logs and jars in hdfs
hdfs dfs -mkdir /spark3-jars
hdfs dfs -mkdir /spark3-logs

# Copy Spark jars to HDFS folder as part of spark.yarn.jars
hdfs dfs -put /opt/spark3/jars/* /spark3-jars

# Integrate Spark with Hive Metastore. Create soft link for hive-site.xml in Spark conf folder
sudo ln -s /opt/hive/conf/hive-site.xml /opt/spark3/conf/

# Install Postgres JDBC jar in Spark jars folder
sudo wget https://jdbc.postgresql.org/download/postgresql-42.2.19.jar -O /opt/spark3/jars/postgresql-42.2.19.jar

# Validate using Scala
/opt/spark3/bin/spark-shell --master yarn --conf spark.ui.port=0
exit by typing :q

# Validate using Python
/opt/spark3/bin/pyspark --master yarn --conf spark.ui.port=0
spark.sql('SHOW databases').show()
spark.sql('USE test')
spark.sql('SELECT * FROM spark').show() 
exit()

# All the commands (spark-shell, pyspark, spark-submit) are available in:
/opt/spark3/bin

# Set these commands path at .profile. Add below 2 lines in .profile
export PATH=$PATH:/opt/spark3/bin

source .profile

# Distigiush these commands in spark3
mv /opt/spark3/bin/pyspark /opt/spark3/bin/pyspark3

# Validate the commands
pyspark3 --master yarn


# Create Spark Job
cd
vi basic.py
print("Start ...")
  
from pyspark.sql import SparkSession
spark = SparkSession \
       .builder \
       .master('yarn') \
       .appName("Python Spark SQL basic example") \
       .getOrCreate()

spark.sparkContext.setLogLevel('OFF')
print("Spark Object is created")
print("Spark Version used is :" + spark.sparkContext.version)

print("... End")

cp /opt/spark3/bin/spark-submit /opt/spark3/bin/spark3-submit

# Submit the Job to the Cluster
spark3-submit --master yarn /home/easylearnspark1/basic.py

# Turn off the INFO logs
cp /opt/spark3/conf/log4j.properties.template /opt/spark3/conf/log4j.properties 
set log4j.rootCategory=INFO, console
to
log4j.rootCategory=WARN, console

spark3-submit --master yarn /home/easylearnspark1/basic.py